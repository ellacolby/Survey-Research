# -*- coding: utf-8 -*-
"""survey_research_personal_PAT_LLM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V_N8SDEtXtdky6BUWAQvPe6uospHDaEM
"""

!pip install openai

import os
import openai
import re
openai.api_key = "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"

from google.colab import drive
drive.mount('/content/drive')

with open("/content/drive/MyDrive/2021 Survey/Recordings/Spencer_Clark_AOS (REPEAT)/GMT20230311-185550_Recording.transcript.vtt", "r") as fd:
  transcript = fd.read()

transcript

def clean_transcript(text, name_a, name_b):

    text = text.replace(name_a, "A")
    text = text.replace(name_b, "B")
    text = re.sub(r'\n\n\d+\n', '', text) # remove \n\n{num}\n\n
    # Regex pattern for the given timestamp format
    pattern = r'\d{2}:\d{2}:\d{2}\.\d{3} --> \d{2}:\d{2}:\d{2}\.\d{3}'
    return re.sub(pattern, '', text)

t = clean_transcript(transcript, "Ziyang Xu", "Spencer Clark")
t

response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": f"Here is a transcript of a research: {t}. What are some insights from this conversation? Please focus on the insights related to computation usage." },
    ]
)

print(response['choices'][0]['message']['content'])

import os

# Initialize an empty list to hold the transcript file paths
transcripts = []

# Walk through the directory structure
for dirpath, dirnames, filenames in os.walk("/content/drive/MyDrive/2021 Survey/Recordings/"):
    for filename in filenames:
        if filename.endswith(".transcript.vtt"):
            # Construct the full path to the file and append to list
            full_path = os.path.join(dirpath, filename)
            transcripts.append(full_path)

print(transcripts)
print(len(transcripts))

print(transcripts[5])

import re
from collections import Counter

# Function to clean the transcript
def clean_transcript(text):
    # Extract names that come before a colon in the text
    name_occurrences = re.findall(r"\n([\w\s\-]+):", text)

    # Count the occurrences of each name and pick the two most frequent
    counter = Counter(name_occurrences)
    most_common_names = counter.most_common(3)

    if len(most_common_names) < 2:
      if len(most_common_names) == 0:
        print("Couldn't identify both names, using defaults 'A' and 'B'")
        name_a, name_b = 'A', 'B'
        # print(text)
      if len(most_common_names) == 1:
        name_a = most_common_names[0][0]
        name_b = 'B'
        print("Only found one name")
    else:
        name_a, name_b = most_common_names[0][0], most_common_names[1][0]

    text = text.replace(name_a, "A")
    text = text.replace(name_b, "B")
    text = re.sub(r'\n\n\d+\n', '', text)
    pattern = r'\d{2}:\d{2}:\d{2}\.\d{3} --> \d{2}:\d{2}:\d{2}\.\d{3}'
    return re.sub(pattern, '', text)

# Initialize an empty list to store cleaned transcripts
cleaned_transcripts = []
raw_transcripts = []

# Loop through each transcript file and clean its content
for transcript_file in transcripts:
    with open(transcript_file, "r") as fd:
        raw_transcript = fd.read()
        raw_transcripts.append(raw_transcript)
        cleaned_text = clean_transcript(raw_transcript)
        cleaned_transcripts.append(cleaned_text)

# Now, cleaned_transcripts contains the cleaned content of all transcript files

# print(raw_transcripts[10])
# print(cleaned_transcripts[10])
print(len(cleaned_transcripts))
# # Calculate the total number of entries in the cleaned_transcripts array
# total_entries = len(cleaned_transcripts)

# # Calculate the sum of the lengths of all cleaned transcripts
# total_length = sum(len(transcript) for transcript in cleaned_transcripts)

# # Calculate the average length of the cleaned transcripts
# average_length = total_length / total_entries if total_entries > 0 else 0

# print(f"The total number of entries is {total_entries}.")
# print(f"The average length of the cleaned transcripts is {average_length}.")

response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo-16k",
    messages=[
        {"role": "system", "content": "You are a helpful research assistant. You will parse through an interview transcript and attempt to analyze the respondent's answers to certain questions."},
        {"role": "user", "content": f"Here is a transcript of a research: {cleaned_transcripts[10]}. The interviewer, 'B' asks the interviewee 'A' the following questions: '7.1 How would your research change with 2x, 10x, 100x speedup of the main computation tasks? (Break it down for 2x/10x/100x, is there a difference?)' This question touches on the idea of the researcher benefiting from a speedup in computational power. For this transcript, can you report whether or not the researcher feels that they would benefit from a 2x speedup, a 10x speed up or a 100x speedup. The researcher also asks: 'Does this project require specific hardware (GPU, TPU, FPGA, etc?)?' You should report whether or not the researcher mentions using GPU or cluster computing in their research, at all throughout the interview. Your answer should sound like this: In this interview, the researcher feels that they (would/would not) benefit from a 2x speedup, that they (would/would not) benefit from a 10x speedup, and that they (would/would not) benefit from a 100x speedup. The researcher (does/does not) use GPUs. The researcher (does/does not) use computer clusters'" },
    ]
)

print(response['choices'][0]['message']['content'])

cleaned_transcripts[2]

# Initialize an empty list to store LLM responses
llm_responses = []

# Define the questions you want to ask

# cleaned_transcript = cleaned_transcripts[10]
# Loop through each cleaned transcript
# if 0 == 0:
for i in range(7):

    # Prompt the LLM with the user message
    response = openai.ChatCompletion.create(
      model="gpt-3.5-turbo-16k",
      messages=[
          {"role": "system", "content": "You are a helpful research assistant. You will parse through an interview transcript and attempt to analyze the respondent's answers to certain questions."},
          {"role": "user", "content": f"Here is a transcript of a research: {cleaned_transcripts[i]}. The interviewer, 'B' asks the interviewee 'A' the following questions: '7.1 How would your research change with 2x, 10x, 100x speedup of the main computation tasks? (Break it down for 2x/10x/100x, is there a difference?)' This question touches on the idea of the researcher benefiting from a speedup in computational power. For this transcript, can you report whether or not the researcher feels that they would benefit from a 2x speedup, a 10x speed up or a 100x speedup. The researcher also asks: 'Does this project require specific hardware (GPU, TPU, FPGA, etc?)?' You should report whether or not the researcher mentions using GPU or cluster computing in their research, at all throughout the interview. Your answer should sound like this: In this interview, the researcher feels that they (would/would not) benefit from a 2x speedup, that they (would/would not) benefit from a 10x speedup, and that they (would/would not) benefit from a 100x speedup. The researcher (does/does not) use GPUs. The researcher (does/does not) use computer clusters'" },
      ]
    )

    # Extract and append the LLM's response to the list
    llm_responses.append(response['choices'][0]['message']['content'])
    print(f"Sucessfully processed transcript: {i}")
    print("------------")

    #print(f"Processed transcript {i+1}/{len(cleaned_transcripts)}")

print(llm_responses[7])
print("-----------------")
print("-----------------")
print("-----------------")
print(llm_responses[8])
print("-----------------")
print("-----------------")
print("-----------------")
print(llm_responses[9])
print("-----------------")
print("-----------------")
print("-----------------")
print(llm_responses[10])
print("-----------------")
print("-----------------")
print("-----------------")
print(llm_responses[11])
print("-----------------")
print("-----------------")
print("-----------------")
print(llm_responses[12])
print("-----------------")
print("-----------------")
print("-----------------")
print(llm_responses[13])

for i in range(7, 14):

    # Prompt the LLM with the user message
    response = openai.ChatCompletion.create(
      model="gpt-3.5-turbo-16k",
      messages=[
          {"role": "system", "content": "You are a helpful research assistant. You will parse through an interview transcript and attempt to analyze the respondent's answers to certain questions."},
          {"role": "user", "content": f"Here is a transcript of a research: {cleaned_transcripts[i]}. The interviewer, 'B' asks the interviewee 'A' the following questions: '7.1 How would your research change with 2x, 10x, 100x speedup of the main computation tasks? (Break it down for 2x/10x/100x, is there a difference?)' This question touches on the idea of the researcher benefiting from a speedup in computational power. For this transcript, can you report whether or not the researcher feels that they would benefit from a 2x speedup, a 10x speed up or a 100x speedup. Then, you should report whether or not the researcher mentions using GPU or cluster computing in their research, at all throughout the interview. Your answer should sound like this: In this interview, the researcher feels that they (would/would not) benefit from a 2x speedup, that they (would/would not) benefit from a 10x speedup, and that they (would/would not) benefit from a 100x speedup. The researcher (does/does not) use GPUs. The researcher (does/does not) use computer clusters'" },
      ]
    )

    # Extract and append the LLM's response to the list
    llm_responses.append(response['choices'][0]['message']['content'])
    print(f"Sucessfully processed transcript: {i}")
    print("------------")

for i in range(10, 17):

    # Prompt the LLM with the user message
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-16k",  # You can choose an appropriate model
        messages=[
            {"role": "system", "content": "You are a helpful research assistant. You will parse through an interview transcript and attempt to analyze the respondent's answers to certain questions."},
            {"role": "user", "content": f"Here is a transcript of a research: {cleaned_transcripts[i]}. The interviewer, 'B' asks the interviewee 'A' the following questions: '4.2 Do you think the computation performance behavior of the project is unsatisfactory?'. Then, the researcher asks: '4.3 Are your bigger research plans unattainable due to lack of computation power?'. Then, the researcher asks: '4.4 Is additional computation blocked by computation speed or lack of memory or something else?' These questions touch on the idea of the researcher being 'blocked' by computation. Can you give a qualitative assessment, based on the transcript, of whether or not the researcher is blocked by computation. If the researcher is blocked, give details that they reports as to why they feel they are blocked by computaiton. If they are not blocked, give the reasoning that they report as to why they feel that they are not blocked by computation. Please write a paragraph summary describing the researcher's situation. Do not give me the researcher's direct answers. Instead, you should summarize the researcher's main positions regarding this issue that they report during their interview. Your answer should begin 'Based on the transcript, the researcher feels that ...' and then give a description of their position. "},
        ]
    )

    # Extract and append the LLM's response to the list
    llm_responses.append(response['choices'][0]['message']['content'])
    print(f"Sucessfully processed transcript: {i}")
    print("------------")

for i in range(73, 75):

    # Prompt the LLM with the user message
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-16k",  # You can choose an appropriate model
        messages=[
            {"role": "system", "content": "You are a helpful research assistant. You will parse through an interview transcript and attempt to analyze the respondent's answers to certain questions."},
            {"role": "user", "content": f"Here is a transcript of a research: {cleaned_transcripts[i]}. The interviewer, 'B' asks the interviewee 'A' the following questions: '4.2 Do you think the computation performance behavior of the project is unsatisfactory?'. Then, the researcher asks: '4.3 Are your bigger research plans unattainable due to lack of computation power?'. Then, the researcher asks: '4.4 Is additional computation blocked by computation speed or lack of memory or something else?' These questions touch on the idea of the researcher being 'blocked' by computation. Can you give a qualitative assessment, based on the transcript, of whether or not the researcher is blocked by computation. If the researcher is blocked, give details that they reports as to why they feel they are blocked by computaiton. If they are not blocked, give the reasoning that they report as to why they feel that they are not blocked by computation. Please write a paragraph summary describing the researcher's situation. Do not give me the researcher's direct answers. Instead, you should summarize the researcher's main positions regarding this issue that they report during their interview. Your answer should begin 'Based on the transcript, the researcher feels that ...' and then give a description of their position. "},
        ]
    )

    # Extract and append the LLM's response to the list
    llm_responses.append(response['choices'][0]['message']['content'])
    print(f"Sucessfully processed transcript: {i}")
    print("------------")

len(cleaned_transcripts[33])

print(llm_responses[0])
print("-----------------")
print("-----------------")
print("-----------------")
print(llm_responses[2])
print("-----------------")
print("-----------------")
print("-----------------")
print(llm_responses[4])
print("-----------------")
print("-----------------")
print("-----------------")
print(llm_responses[73])

print(llm_responses)

for i in range (20, 40):
  print(f"Transcript # {i}")
  print(llm_responses[i])
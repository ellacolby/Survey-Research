{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huOZhvqaQiMD",
        "outputId": "6ceddd41-d41d-475a-af88-690f779d7eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.6-cp310-cp310-macosx_11_0_arm64.whl (347 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.7/347.7 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.20\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.3.1-cp310-cp310-macosx_11_0_arm64.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.2/118.2 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-macosx_11_0_arm64.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-macosx_11_0_arm64.whl (29 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.4.0-cp310-cp310-macosx_11_0_arm64.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs>=17.3.0\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, tqdm, multidict, idna, frozenlist, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 certifi-2023.7.22 charset-normalizer-3.3.1 frozenlist-1.4.0 idna-3.4 multidict-6.0.4 openai-0.28.1 requests-2.31.0 tqdm-4.66.1 urllib3-2.0.7 yarl-1.9.2\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Explain how you handled disputes that arose.\n",
            "\n",
            "Tell me about an activity you have participated in and the impact it had.\n",
            "\n",
            "What type of employer are you interested in and why?\n",
            "\n",
            "How do you deal with pressure on\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "prompt = \"Tell me a story about a time you were in a group project.\"\n",
        "response = openai.Completion.create(\n",
        "    engine=\"davinci\",\n",
        "    prompt=prompt,\n",
        "    max_tokens=50\n",
        ")\n",
        "\n",
        "print(response.choices[0].text.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Iq2mklz2Q0TW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W18-EccQ9xm",
        "outputId": "6e19c1c0-57a7-49ce-f1ae-6babcd773cf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZUem6lVRq65"
      },
      "outputs": [],
      "source": [
        "transcript = []\n",
        "# python users\n",
        "with open(\"/content/drive/MyDrive/2021 Survey/Recordings/Cathy_Wang_CEE/GMT20230201-180135_Recording.transcript.vtt\", \"r\") as fd:\n",
        "  transcript.append(fd.read())\n",
        "with open(\"/content/drive/MyDrive/2021 Survey/Recordings/Amy_Secunda_Astro (REPEAT)/GMT20230310-180451_Recording.transcript.vtt\", \"r\") as fd:\n",
        "  transcript.append(fd.read())\n",
        "with open(\"/content/drive/MyDrive/2021 Survey/Recordings/Jackson_Deobald_Chem/GMT20230131-163124_Recording.transcript.vtt\", \"r\") as fd:\n",
        "  transcript.append(fd.read())\n",
        "with open(\"/content/drive/MyDrive/2021 Survey/Recordings/Sev_Harootonian_Psychology/GMT20230203-180347_Recording.transcript.vtt\", \"r\") as fd:\n",
        "  transcript.append(fd.read())\n",
        "with open(\"/content/drive/MyDrive/2021 Survey/Recordings/Yinan_Qiu_Econ/GMT20230222-150313_Recording.transcript.vtt\", \"r\") as fd:\n",
        "  transcript.append(fd.read())\n",
        "with open(\"/content/drive/MyDrive/2021 Survey/Recordings/Andrew_Ferdowsian_ECON/GMT20230124-200158_Recording.transcript.vtt\", \"r\") as fd:\n",
        "  transcript.append(fd.read())\n",
        "# non python users\n",
        "with open(\"/content/drive/MyDrive/2021 Survey/Recordings/Guanyu_Liao_Chem/GMT20230125-163259_Recording.transcript.vtt\", \"r\") as fd:\n",
        "  transcript.append(fd.read())\n",
        "with open(\"/content/drive/MyDrive/2021 Survey/Recordings/Stephen_Jardin_PPPL /GMT20230309-132956_Recording.transcript.vtt\", \"r\") as fd:\n",
        "  transcript.append(fd.read())\n",
        "with open(\"/content/drive/MyDrive/2021 Survey/Recordings/Clement_Ekaputra_Northwestern_MaterialScience/GMT20230324-160147_Recording.transcript.vtt\", \"r\") as fd:\n",
        "  transcript.append(fd.read())\n",
        "with open(\"/content/drive/MyDrive/2021 Survey/Recordings/Noe-Heon_Kim_GEO/GMT20230316-140216_Recording.transcript.vtt\", \"r\") as fd:\n",
        "  transcript.append(fd.read())\n",
        "with open(\"/content/drive/MyDrive/2021 Survey/Recordings/Erica_Lai_ORFE/GMT20230131-143145_Recording.transcript.vtt\", \"r\") as fd:\n",
        "  transcript.append(fd.read())\n",
        "with open(\"/content/drive/MyDrive/2021 Survey/Recordings/Jessica_Gaetgens_Chem/GMT20230224-160253_Recording.transcript.vtt\", \"r\") as fd:\n",
        "  transcript.append(fd.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-wfJbbkSLxJ"
      },
      "outputs": [],
      "source": [
        "def clean_transcript(text, name_a, name_b):\n",
        "\n",
        "    text = text.replace(name_a, \"A\")\n",
        "    text = text.replace(name_b, \"B\")\n",
        "    text = re.sub(r'\\n\\n\\d+\\n', '', text) # remove \\n\\n{num}\\n\\n\n",
        "    # Regex pattern for the given timestamp format\n",
        "    pattern = r'\\d{2}:\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}:\\d{2}\\.\\d{3}'\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "cleaned_transcripts = []\n",
        "\n",
        "# python users\n",
        "cleaned_transcripts.append(clean_transcript(transcript[0], \"Ziyang Xu\", \"Cathy Wang\"))\n",
        "cleaned_transcripts.append(clean_transcript(transcript[1], \"Ziyang Xu\", \"Amy Secunda\"))\n",
        "cleaned_transcripts.append(clean_transcript(transcript[2], \"Ziyang Xu\", \"Jackson Deobald\"))\n",
        "cleaned_transcripts.append(clean_transcript(transcript[3], \"Ziyang Xu\", \"Sev Harootonian\"))\n",
        "cleaned_transcripts.append(clean_transcript(transcript[4], \"Ziyang Xu\", \"Yinan\"))\n",
        "cleaned_transcripts.append(clean_transcript(transcript[5], \"Ziyang Xu\", \"Andrew Ferdowsian\"))\n",
        "\n",
        "# non python users\n",
        "cleaned_transcripts.append(clean_transcript(transcript[6], \"Ziyang Xu\", \"Guanyu Liao\"))\n",
        "cleaned_transcripts.append(clean_transcript(transcript[7], \"Ishita Chaturvedi\", \"Stephen Jardin\"))\n",
        "cleaned_transcripts.append(clean_transcript(transcript[8], \"Enrico Deiana\", \"Clement Ekaputra\"))\n",
        "cleaned_transcripts.append(clean_transcript(transcript[9], \"Ziyang Xu\", \"Noe-Heon Kim\"))\n",
        "cleaned_transcripts.append(clean_transcript(transcript[10], \"Ziyang Xu\", \"Erica Lai\"))\n",
        "cleaned_transcripts.append(clean_transcript(transcript[11], \"Ziyang Xu\", \"Jessica Gaetgens\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCeHU-PWVzQv",
        "outputId": "289548f9-9f4f-4829-a406-cc6efd6379c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.324-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain)\n",
            "  Downloading langsmith-0.0.52-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.324 langsmith-0.0.52 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "X1qXYt-ASSVv",
        "outputId": "17fcd9c2-20c7-4f3b-ecb5-119b30670a77"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a0f1000e3252>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtranscript\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned_transcripts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msplit_transcript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkdown_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msplit_transcripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_transcript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/text_splitter.py\u001b[0m in \u001b[0;36msplit_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# Check each line against each of the header types (e.g., #, ##)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders_to_split_on\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m                 \u001b[0;31m# Check if line starts with a header that we intend to split on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                 if stripped_line.startswith(sep) and (\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "headers_to_split_on = cleaned_transcripts\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "# Create a list to store the split transcripts\n",
        "split_transcripts = []\n",
        "\n",
        "for transcript in cleaned_transcripts:\n",
        "    split_transcript = markdown_splitter.split_text(transcript)\n",
        "    split_transcripts.append(split_transcript)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "kONA9a7tV5cg",
        "outputId": "e1d69fa4-56fe-4c52-87f7-7f70cc59f76b"
      },
      "outputs": [
        {
          "ename": "InvalidRequestError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-697ecdee3f02>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtranscript\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned_transcripts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         messages=[\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             return (\n\u001b[0;32m--> 710\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    711\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    776\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 7572 tokens. Please reduce the length of the messages."
          ]
        }
      ],
      "source": [
        "for transcript in cleaned_transcripts:\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {transcript}. The interviewer, 'B' asks the interviewee 'A' the following question: '2.7 Do you usually write software to automate mundane tasks (like transferring data, making plots)?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\"},\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85TXmrrFjcdL"
      },
      "outputs": [],
      "source": [
        "# python users \"what percent of time is spent waiting on run\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44EdyeBdeZoM"
      },
      "outputs": [],
      "source": [
        "q1response0 = openai.ChatCompletion.create(\n",
        "   model=\"gpt-4\",\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "       {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[0]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7sVi6eCSVK1",
        "outputId": "0b636423-7925-4f61-b841-6cbdaf30a814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The interviewee 'A' responded that a very limited amount of their research time is spent actively waiting for the runs to finish. They haven't proceeded to the stage where waiting would be significant, so currently it is less than 5% of their research time.\n",
            "\n",
            "The exact lines said by the interviewee in response to the question are:\n",
            "\"B: Very limited\n",
            "B: is that I haven't like it.\n",
            "B: Proceed to that stage. Yeah.\n",
            "B: So you say, like less than 5% for now. Yeah, yeah, less than 5% definitely\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(q1response0['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiISym10wCB9"
      },
      "outputs": [],
      "source": [
        "q1response1 = openai.ChatCompletion.create(\n",
        "   model=\"gpt-4\",\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "       {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[1]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W6dnE9ewGuN",
        "outputId": "d27be069-8ee6-428b-ac94-b4210bc592ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The interviewee, 'A', responded that about 90% of their research time is spent actively waiting for runs. The exact lines from the transcript are: \n",
            "\n",
            "\"B: Yeah, I mean that that's probably 90, you know. Because I yeah, I mean, I've plot some interim things and often find. Oh, it's not working, you know. But I think if it was working working, then 90%, because got it so.\"\n"
          ]
        }
      ],
      "source": [
        "print(q1response1['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXIGiXkzwIRL"
      },
      "outputs": [],
      "source": [
        "q1response2 = openai.ChatCompletion.create(\n",
        "   model=\"gpt-4\",\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "       {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[2]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h58uaUQkwKXy",
        "outputId": "27c15dfd-8d07-430a-be14-2da2f5758500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The interviewee 'A' responded that not too much research time is spent on waiting for runs. They usually have something else that is more of the bottleneck. However, in order to run all of the quantum calculations, it takes about a month using up their quota on Arjuna. When asked to quantify in percentage terms, the interviewee estimates that it's probably less than 5%.\n",
            "\n",
            "The interviewee's exact response lines are:\n",
            "\n",
            "B: I guess so.\n",
            "B: Not not too much. I I usually have\n",
            "B: something else I\n",
            "B: the that is, the more of the bottleneck. But\n",
            "B: in in order to to kind of run all of the\n",
            "B: the\n",
            "B: quantum calculations it it takes about\n",
            "B: maybe a month or so\n",
            "B: using up my my quota on on address.\n",
            "A: Got it, and would like, Would you describe that you active waiting time is less than 5%\n",
            "B: or like\n",
            "A: more than that.\n",
            "B: I would say.\n",
            "B: Yeah, probably less than 5%\n"
          ]
        }
      ],
      "source": [
        "print(q1response2['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1zpLIRDcas5"
      },
      "outputs": [],
      "source": [
        "q1response3 = openai.ChatCompletion.create(\n",
        "   model=\"gpt-4\",\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "       {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[3]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGHmqR0EcdAa",
        "outputId": "0ba579c6-f1d4-49f9-bec7-52dce8f575e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The interviewee 'A' responded that he spends \"Not too much\" time waiting for runs, specifically \"maybe 5 to 10%\" of his research time. Here are the exact lines that the interviewee 'A' said to answer the question:\n",
            "\n",
            "\"B: Not too much.\n",
            "B: I I want to say maybe 5 to 10%.\"\n"
          ]
        }
      ],
      "source": [
        "print(q1response3['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxJHyR7fc7cY"
      },
      "outputs": [],
      "source": [
        "q1response4 = openai.ChatCompletion.create(\n",
        "   model=\"gpt-4\",\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "       {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[4]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72sA7QFDc5Vz",
        "outputId": "22d0669c-f223-4025-92b4-378d91681e46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The interviewee 'A' didn't directly respond to the question 'What percentage of research time is spent waiting for runs?' The closest response was when discussing the percentage of time spent on programming and debugging. The line A said was:\n",
            "\n",
            "A: \"80%, because it's mostly a data on smooth.\"\n",
            "\n",
            "The other bit of information that might be relevant is what interviewee 'A' said about how much time is spent on optimization:\n",
            "\n",
            "A: \"I don't I Haven't: I don't need that yet.\" \n",
            "\n",
            "However, both these lines are not direct responses to the question asked in the prompt. It appears that the exact question about waiting for runs was not asked in the interview.\n"
          ]
        }
      ],
      "source": [
        "print(q1response4['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "FQco51Goc98v"
      },
      "outputs": [],
      "source": [
        "q1response5 = openai.ChatCompletion.create(\n",
        "   model=\"gpt-4\",\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "       {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[5]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csiTt1xEc_11",
        "outputId": "25f176e0-cb51-4767-d93b-bc963caf8b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The interviewee 'A' responds that depending on the context, the percentage of research time spent waiting for runs varies. If the context is when he is working on code, then it is about 20 to 30%. If the context is taken as a larger picture of his whole research process, then it accounts for less than 1%.\n",
            "\n",
            "The exact lines the interviewee said to answer the question are: \n",
            "B: \"I would so like, I said I normally don't work on code in the first place. So conditional on me working on code, probably on the order of 20 to 30%. But if you don't condition on that, less than 1%\"\n"
          ]
        }
      ],
      "source": [
        "print(q1response5['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkedazdidFVt"
      },
      "outputs": [],
      "source": [
        "# non python users \"what percent of time is spent waiting on runs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1LG0n26eNJX"
      },
      "outputs": [],
      "source": [
        "q2response0 = openai.ChatCompletion.create(\n",
        "   model=\"gpt-4\",\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "       {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[6]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJIVkLMvlR6f",
        "outputId": "0f0cdca4-13db-4995-e9ba-fb0554876d2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The interviewee 'A' responded to the question 'What percentage of research time is spent waiting for runs?' by saying: \"That's some 1%\". The exact line from the transcript is: \"That's some 1%\"\n"
          ]
        }
      ],
      "source": [
        "print(q2response0['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "IzYg5rPHlSxr",
        "outputId": "13b12690-0e21-4f94-89ec-18c20b5e4726"
      },
      "outputs": [
        {
          "ename": "RateLimitError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-f51c5888dac4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m q2response1 = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    messages=[\n\u001b[1;32m      4\u001b[0m        \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"Here is a transcript of a research: {cleaned_transcripts[7]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\"\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             return (\n\u001b[0;32m--> 710\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    711\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    776\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for gpt-4 in organization org-OELaxRgy25tDQ34qrHVyFsPY on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues."
          ]
        }
      ],
      "source": [
        "q2response1 = openai.ChatCompletion.create(\n",
        "   model=\"gpt-4\",\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "       {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[7]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0bOHfNylVX0"
      },
      "outputs": [],
      "source": [
        "print(q2response1['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkRdruXBlUMp"
      },
      "outputs": [],
      "source": [
        "q2response2 = openai.ChatCompletion.create(\n",
        "   model=\"gpt-4\",\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "       {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[8]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqm6kZFjlmQA",
        "outputId": "11f3baa4-8559-4fde-a1dd-c6b904fde9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The interviewee, 'B', responded that the research time spent waiting for runs is on the order of 30 minutes to 2 hours. The specific line they said was: \"on the order of like 30 min to 2 h.\"\n"
          ]
        }
      ],
      "source": [
        "print(q2response2['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iPbuSU6la2k"
      },
      "outputs": [],
      "source": [
        "q2response3 = openai.ChatCompletion.create(\n",
        "   model=\"gpt-4\",\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "       {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[9]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnP41NXBlq7O",
        "outputId": "605492f6-c625-4b44-e455-b8ead49c8ffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The interviewer (labeled as 'A') did not ask the interviewee (labeled as 'B') the specific question \"What percentage of research time is spent waiting for runs?\" in the provided interview transcript. Therefore, it's not possible to provide the interviewee's answer to this question or to provide the exact lines in the interview where the interviewee responded to this particular question.\n"
          ]
        }
      ],
      "source": [
        "print(q2response3['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "K95bbxcLlbol",
        "outputId": "ccd1e1a9-87fd-4b9e-e38a-e164ea26b1d7"
      },
      "outputs": [
        {
          "ename": "Timeout",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    846\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/util.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             raise ReadTimeoutError(\n\u001b[0m\u001b[1;32m    372\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Read timed out. (read timeout={timeout_value})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             result = _thread_context.session.request(\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_InvalidHeader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTimeout\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-bd8544a03594>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m q2response4 = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    messages=[\n\u001b[1;32m      4\u001b[0m        \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"Here is a transcript of a research: {cleaned_transcripts[10]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\"\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:\n\u001b[0;32m--> 289\u001b[0;31m         result = self.request_raw(\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    615\u001b[0m             )\n\u001b[1;32m    616\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Request timed out: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             raise error.APIConnectionError(\n",
            "\u001b[0;31mTimeout\u001b[0m: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)"
          ]
        }
      ],
      "source": [
        "q2response4 = openai.ChatCompletion.create(\n",
        "   model=\"gpt-4\",\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "       {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[10]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZCDZ1ghlsCV"
      },
      "outputs": [],
      "source": [
        "print(q2response4['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYNp64xqlb-f"
      },
      "outputs": [],
      "source": [
        "q2response5 = openai.ChatCompletion.create(\n",
        "   model=\"gpt-4\",\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "       {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[11]}. The interviewer, 'B' asks the interviewee 'A' the following question: 'What percentage of research time is spent waiting for runs?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb3WXQQwltno"
      },
      "outputs": [],
      "source": [
        "print(q2response5['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7rNz8R7Scvi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Initialize an empty list to hold the transcript file paths\n",
        "transcripts = []\n",
        "\n",
        "# Walk through the directory structure\n",
        "for dirpath, dirnames, filenames in os.walk(\"/content/drive/MyDrive/2021 Survey/Recordings/\"):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith(\".transcript.vtt\"):\n",
        "            # Construct the full path to the file and append to list\n",
        "            full_path = os.path.join(dirpath, filename)\n",
        "            transcripts.append(full_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnTo8WhggMc9",
        "outputId": "023bb5bd-aaef-4b10-918c-0463f2e37b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/content/drive/MyDrive/2021 Survey/Recordings/Michel-Mata_EEB/GMT20220921-210131_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Hamann_MATH/GMT20221028-220543_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Li_ORFE/GMT20221102-173130_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/patino_northwestern_ME/GMT20221109-220512_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Hua_CHEM/GMT20221030-190018_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Cui_GEO/GMT20220926-210115_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Zexuan_Zhong_COS/GMT20221202-181010_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Rose_Guingrich_Pyschology/GMT20230117-190118_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Tetsekela_Anyiam-Osigwe_Politics/GMT20230123-200244_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Jessica_Ye_PNI/GMT20230124-180225_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Andrew_Ferdowsian_ECON/GMT20230124-200158_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Guanyu_Liao_Chem/GMT20230125-163259_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Warren_Yuan_CEE/GMT20230127-160234_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Erica_Lai_ORFE/GMT20230131-143145_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Jackson_Deobald_Chem/GMT20230131-163124_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Gregory_Dobbels_Econ/GMT20230131-180227_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Wyatt_Suling_SPIA/GMT20230131-190015_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Norah_Woodcock_Philosophy/GMT20230201-160619_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Cathy_Wang_CEE/GMT20230201-180135_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Fan_Wu_EAS/GMT20230202-160341_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Felipe_Oliveira_SPIA/GMT20230202-200218_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Pooja_Ramamurthi_SPIA/GMT20230202-210556_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Nong_Li_SPIA/GMT20230203-160005_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Sev_Harootonian_Psychology/GMT20230203-180347_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Daniel_Multer_Chem/GMT20230217-000020_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Eric_Qian_Econ/GMT20230217-200006_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Yinan_Qiu_Econ/GMT20230222-150313_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Rajat_Joshi_AOS/GMT20230223-150052_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Albert_Chung_EEB/GMT20230224-140307_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Jessica_Gaetgens_Chem/GMT20230224-160253_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Guangye_Zhou_CEE/GMT20230225-201406_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Ben_Israeli_Astro/GMT20230301-190203_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Nathaniel_Ferraro_PPPL/GMT20230303-160147_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Sigurd_Næss_ASTRO/GMT20230303-190015_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Leander_Thiele_Phy/GMT20230305-020135_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Mathieu_Poupon_AOS/GMT20230306-140903_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Enas_Hamdy_EEB/GMT20230306-145945_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Wenchang_Yang_GEO/GMT20230306-163204_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Pranay_Manocha_CS/GMT20230307-195748_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Shashank_Anand_GEO/GMT20230307-212344_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Kai-Yuan_Cheng_AOS/GMT20230309-133425_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Stephen_Jardin_PPPL /GMT20230309-132956_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Ziyang_Wei_MAE/GMT20230309-150151_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Hang_Zhang_Chem/GMT20230309-200317_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Debra_Keiser_Chem/GMT20230310-130327_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Arno_Vanthieghem_Astro/GMT20230310-203228_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Amy_Secunda_Astro (REPEAT)/GMT20230310-180451_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Spencer_Clark_AOS (REPEAT)/GMT20230311-185550_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Bhishma_Dedhia_ECE/GMT20230312-005841_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Brian_Wynne_MAE/GMT20230314-180414_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Jarome_Ali_EEB (REPEAT)/GMT20230314-210055_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Ameet_Deshpande_COS/GMT20230315-145302_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Satyen_Dhamaker_CBE/GMT20230315-131019_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Abhishek_Biswas_Molbio/GMT20230315-163205_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Charles_Maher_Chem/GMT20230315-200218_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Stephanie_Monson_Chem/GMT20230316-000334_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Noe-Heon_Kim_GEO/GMT20230316-140216_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Vinh_San_Dinh_Northwestern (REPEAT)/GMT20230316-150225_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Troy_Comi_ResearchComputing CBE/GMT20230316-180304_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Pablo_Oyler-Castrillo_QCB/GMT20230317-150655_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Albert_Mollen_PPPL/GMT20230317-180128_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Abla_Alaoui-Soce_Psychology/GMT20230317-190430_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Michael_Churchill_PPPL/GMT20230317-193407_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Colton_Bryant_Northwestern_AppliedMath/GMT20230322-200201_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Ian_Madden_Northwestern_MaterialScience/GMT20230324-150143_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Clement_Ekaputra_Northwestern_MaterialScience/GMT20230324-160147_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Thomas_Berrueta_Northwestern_MechEng/GMT20230327-150203_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/David_Burns_Northwestern_MechEng/GMT20230327-180209_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Jack_Kolberg-Edelbrock_Northwestern_MaterialScience/GMT20230329-160435_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Suman_Bhandari_Northwestern_MechEng/GMT20230330-160713_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Mark_Noll_Northwestern_IEMS/GMT20230330-211905_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Nathan_Koocher_Northwestern_MaterialScience/GMT20230331-160224_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Jannat_Ahmed_Northwestern_MechEng/GMT20230331-193246_Recording.transcript.vtt', '/content/drive/MyDrive/2021 Survey/Recordings/Ethan_Suwandi_Northwestern_MaterialScience/GMT20230331-200339_Recording.transcript.vtt']\n"
          ]
        }
      ],
      "source": [
        "print(transcripts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4piPbVvixYW"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Function to clean the transcript\n",
        "def clean_transcript(text):\n",
        "    # Extract names that come before a colon in the text\n",
        "    name_occurrences = re.findall(r\"\\n([A-Za-z\\s]+):\", text)\n",
        "\n",
        "    # Count the occurrences of each name and pick the two most frequent\n",
        "    counter = Counter(name_occurrences)\n",
        "    most_common_names = counter.most_common(3)\n",
        "\n",
        "    if len(most_common_names) < 2:\n",
        "      if len(most_common_names) == 0:\n",
        "        print(\"Couldn't identify both names, using defaults 'A' and 'B'\")\n",
        "        name_a, name_b = 'A', 'B'\n",
        "        # print(text)\n",
        "      if len(most_common_names) == 1:\n",
        "        name_a = most_common_names[0][0]\n",
        "        name_b = 'B'\n",
        "    else:\n",
        "        name_a, name_b = most_common_names[0][0], most_common_names[1][0]\n",
        "\n",
        "    text = text.replace(name_a, \"A\")\n",
        "    text = text.replace(name_b, \"B\")\n",
        "    text = re.sub(r'\\n\\n\\d+\\n', '', text)\n",
        "    pattern = r'\\d{2}:\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}:\\d{2}\\.\\d{3}'\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "# Initialize an empty list to store cleaned transcripts\n",
        "cleaned_transcripts = []\n",
        "\n",
        "# Loop through each transcript file and clean its content\n",
        "for transcript_file in transcripts:\n",
        "    with open(transcript_file, \"r\") as fd:\n",
        "        raw_transcript = fd.read()\n",
        "        cleaned_text = clean_transcript(raw_transcript)\n",
        "        cleaned_transcripts.append(cleaned_text)\n",
        "\n",
        "# Now, cleaned_transcripts contains the cleaned content of all transcript files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me9xgqc9i4Se",
        "outputId": "485f5448-ef99-42c1-926d-78e182ec46fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WEBVTT\n",
            "A: In the chemical biology field. So basically I'm: I'm mostly working on protein chemistry\n",
            "A: Uh: mostly designing new methods to do the uh new method approaching. And\n",
            "A: yeah, that's what i'm doing in my lab,\n",
            "B: I see. So is that, like sort of the high level goals of the reason lockdown research? But is that a project working on right now?\n",
            "B: Is that the available goal of your so? Is that a project you're working on right now?\n",
            "A: I found this on the web for is that a high level goal of a resource? Check it out.\n",
            "A: So yeah, my my own projects may. It's basically you know. I will not do a lot of thing about protein, and my my main focus is on the methodology of how to end a new approach.\n",
            "A: Yeah.\n",
            "Okay.\n",
            "B: So how long have you been working on this project for\n",
            "A: only three years?\n",
            "B: Yeah, I see. So have you like built on this project incrementally or uh so like\n",
            "A: adding one goal over another. Or is this like the\n",
            "A: Oh, it's actually like a previous post off that some idea, and say we should try it. I don't know. I I just enter that, and we start trying, and that's the idea of work. So I stick to, I think, working on that idea for the entire time\n",
            "A: and the like. You know the the entire mass of the system, and the find the application for this like very standardable for for the for the chemistry stuff.\n",
            "A: So technically my project is So we I don't really use computational method. But, uh you certain application, for example, when we are we\n",
            "A: uh the way we want to study? So study that stuff stuff like a pretty important interaction. So that's more like in about a priority side. In that case, because we need to use nice spectrometry method, we should give you tons of data, and in that case,\n",
            "A: uh people are start with you coding uh, So start to start recording mass over the war on certain program to do that. I couldn't not engage with it. But a lot of people, you know, are not actually doing that, and I may gonna start to use it in the near future. Once I like uh\n",
            "A: it, pass it to one of an application project, but it is still at a very, very preliminary for you. So in the future I may also gonna start a user\n",
            "B: I see. So um have before this project. Have you been using programming in your other projects?\n",
            "A: Well, no like. Actually, I found this on the web for using programming and your other project is check it out.\n",
            "B: I don't know what to come with you today. I am not speaking with Yeah, so um is, is it? Have you used programming any any previous projects?\n",
            "A: No, but mostly just you at the chemistry and the didn't didn't touch any area that I could\n",
            "B: like. It requires such a huge amount of data processing. So I never actually use it. So in like Cb. Do you usually like to need programming methods only to process a lot of data. Otherwise, Are you mostly theoretical and experimental?\n",
            "A: So yeah, that's another thing. So\n",
            "A: I usually happen to you sort of product like, uh, you know, make your new protein. So you need to uh predict, predict the structure, or like\n",
            "A: like certain interaction between these are molecules. In that case we may gonna use certain like open sourcesource open source program on on, on the Internet for for those purpose. Uh, for example, the offer phones, as they say, pretty popular now, like you know, they everybody. But they want to\n",
            "A: make a figure of the protein, and they don't already have a like a available available structural software data. They just you have people to the offer for for a sequence, and they will. They will export the entire like predicate structure for us.\n",
            "A: That's that's a pretty common thing that we we do. I also do that a lot, because that is a pretty fast way to get a pretty good looking figure,\n",
            "A: and the other side is the I think the other thing I'm familiar with is It's still about like a huge amount of data processing like in a mass mass spectrometry in about like twenty-four interaction, because you're basically uh dealing with solutions. Or protein. So clearly, you're gonna need to call\n",
            "A: to get a data,\n",
            "A: but i'm just not familiar with it, because I got people. You know what I was doing, but\n",
            "A: I see so in high school under that. Did you use any programming languages? Or was it just something that you did not? Well, uh, for I did I I I have a lot of circle and program programming. But uh\n",
            "A: it just for the like cross requirement. But I do. I really like too much into them.\n",
            "A: So yeah, it basically has to your experience, I can say that. Which which language did you learn, though? Uh, c plus, I don't. I don't even know the English name, c. Plus for us. It Doesn't:\n",
            "A: right? Yeah, Yeah, It's it to see that that's that's where I, my mainly learn people.\n",
            "I see.\n",
            "B: I'm listening.\n",
            "A: Some kind of ai system there.\n",
            "B: So um in your projects. Do you work on like? Do you work alone, or do you work with the whole group of?\n",
            "A: I used to have the post office, but the person who actually gave that idea of uh he left after one year. So I basically just working on my own.\n",
            "I see.\n",
            "Um,\n",
            "let me.\n",
            "B: Which which uh So in your lab in general.\n",
            "B: So what percentage of people use a lot of\n",
            "A: at least five people who are actually using code into to analyze data.\n",
            "A: Yeah, it's probably more than that. But I\n",
            "A: and at least five we are. We are roughly, more than twenty people. That yeah, something that\n",
            "I see.\n",
            "Um.\n",
            "A: So we actually have a new call for. Used to be uh, I think, is a theoretical chemist at at that side. So, probably because he just arrived, but like, so we'd be there. We don't really know what the exact product is gonna work on now, but I can't see that since,\n",
            "A: like my Pi does that's kill him here, so we could probably do a lost more in the future,\n",
            "B: have you? I mean, I don't know. Have you ever like attended any special training to get used to those tools that you use uh for programming? Have you have, like any seminars, workshops, tutorials\n",
            "A: uh I person never drawing room, but I think they are like a seminar in in the department posing chemistry and the mobile that they have some trouble with whether a seminar or a workshop. But that's a train you how to use uh like data session data processing As a\n",
            "A: I I think they I think they are, so to say, a second crossing up all this I I remember I saw the email before, but I I I never to kick into the because at that time I don't. I don't have a needs for them. So yeah,\n",
            "B: I see. So when was the last time that you learned a new programming language.\n",
            "A: Oh, that's a long time ago. It's it's gonna be. It's gonna be my third year in my underground. So it's already.\n",
            "A: I don't know five years ago, I guess.\n",
            "B: And how long did it take for you to get comfortable with it?\n",
            "A: Uh, It's all familiar,\n",
            "A: I think. Just for the course it's for the course requirements. They it's pretty easy to just get used to how hard to use a certain number. It's not not that hard. I say\n",
            "A: I, it's it's because maybe if I I actually learn a certain I don't remember which longer is but that I have taken\n",
            "A: uh coding courses that you know, all the way back to high school, so probably I've already been there with us, so we\n",
            "A: to try to like, understand all the things.\n",
            "A: So it means I I I think, if I need to learn not a new language in the future, it probably not gonna be a big deal. If just\n",
            "A: how would I? I can just like a self learning from you, and I was on.\n",
            "A: Do you use any? Do you know any other programming languages apart from C. I'm see. Uh, I mean, I I know room like python, just like so kind of zoom. But I just I don't know how to use it. I just for other people using that a lot.\n",
            "A: Uh I don't think there's any specific reason you just to cross it there. So yeah,\n",
            "B: I see. So do you write code for plotting? So whether we need to make plots. Do you use any programming language to make it? Or do you use a?\n",
            "A: I I don't. I don't even know what the code is to generate certain certain uh figures, but I never. I know I've never done that. And the probably because my data, the current is the the current data I have. Is not that complicated? So, like a directly product of the that just makes figure out, always using, excel, or\n",
            "A: or like a or a general something that's just that's still there already, like Very sufficient for me.\n",
            "A: So I don't know\n",
            "B: which operating system do you use on a daily basis?\n",
            "A: Oh, uh, you mean my computer?\n",
            "B: Okay? And do you also have like a dual port with Linux or do you?\n",
            "B: So I mean, you don't usually\n",
            "B: good. So\n",
            "A: and of course there's no battleism. I'm just like going through my questions and seeing what would.\n",
            "A: Yeah, the yeah. I just feel weird for me because I just don't. I just don't do that. That a lot.\n",
            "Ziyang Xu: Yeah, don't worry. We We pick you up people from like English departments uh politics, like all kind all all around. So we can. Hopefully, we get a good picture of everyone That's I don't know what? What percentage computation do they use?\n",
            "Yeah,\n",
            "B: Good.\n",
            "B: When you do, code, do you look for progress uh portability or a and reusability, or to say, call it out for the time being, and then, just like That's it.\n",
            "A: So would you like to come back to your code and leave a lot of comments in it, so that when you come back like three years down the line and like, all right,\n",
            "A: Yeah, I don't know how I think I have to actually do this. I'm going to see this for you, too.\n",
            "B: So what what is the main purpose of your research project? I'm: i'm not from chemistry. So I don't completely understand.\n",
            "A: But I mean, if I talk about I can't talk about it.\n",
            "A: I want to figure out so I want. I want to. There's one wife. Uh, the Actually, our lab is to develop like new, I said, Protein and Nuri and the the ball how to call you, Since since I've smaller protein and programming. Then they can me to a bigger and a full protein like we call Porting Negation\n",
            "A: and my I my uh, but there's a problem with that is that because your protein has your protein can be very be, you can have several hundred under the has, and the the chemical teams as a master, that for making them is very limited. You don't really want to go over fifty amino acid,\n",
            "A: so my project needs to solve the problem of this this side problem. I want to see out a way to directly uh replace a sequence in the middle of a protein into something that I want to load in there.\n",
            "A: It's kind of similar to how we do the cloning. So in the end we we have recombination that directly swap us. Dna sequence into something else.\n",
            "A: And uh and i'm trying to figure out a way to do a protein recombination so directly replacing a sequence in the middle of,\n",
            "A: and that may be very sort of ground baking, because you already, if you want to make a very lot of it, you need to, uh,\n",
            "A: just that. The chop it into several pieces and put them back to them together. It's a very long process, and it's not difficult, because protein need to fall,\n",
            "A: and if you if you travel with several pieces that usually like ninety percent of the time they just phone for. So it's very difficult. So if I can just directly replace some secret in the middle. Then I will avoid all those kind of problem. And that's basically what i'm doing right now,\n",
            "B: I see. And you'd not So partly not just programming. Do you use any software? And you work on this from very basic one. It just excel whatever you think so. And uh, also for\n",
            "A: because I make a protein sometime, why, it does directly express I mean a cell. So in that case I need something.\n",
            "A: Uh, for example, Snapchat. If it's a it's a It's a very commonly used program for us to look into the Dna to look into the Dna sequence of a of a protein, so we can add it. So\n",
            "A: So when you want to name mutation, I will use that software\n",
            "A: like making different, add it in the entire sequence.\n",
            "A: And also I, as I mentioned,\n",
            "A: that thing is our open source uh, uh, like open source uh website that we just put in the sequence in the give. Give us a for you from on a predictive structure and a very useful\n",
            "A: I so that you just become popular with like a few. I think it's just a few months ago, and uh, I believe everybody also using it.\n",
            "A: And another program is called Timel. It's A. It's also like a bit free program that you can download from being on that.\n",
            "But that same way,\n",
            "A: uh will automatically display the structural approach here and the\n",
            "A: But basically if you have already have a information, or you can uh this predictive software from the from the off of phone and put it in there. It will display the infrastructure. So you can look at the some details on the structure that you go. If that was your\n",
            "A: it's actually a small molecule interaction. We're probably it's it's very useful.\n",
            "A: And I think that's the major seven uh the program that mostly use\n",
            "B: and are all of these uh software publicly available.\n",
            "A: Yes, uh for Snapchat. I think this university five uh\n",
            "A: by the license. So I think we just uh download it from. I don't remember the departments. They were all it. I don't remember, but we did not directly download it,\n",
            "A: and for the the others are mostly already like free, free for that of uh free to use, because they are all like uh\n",
            "A: made by other reservoirs and all like open source or something. So don't have any problem accessing it. Just click download, and then you get it. So Is this source code also publicly available, or\n",
            "I I i'm not sure that I I don't know about that, for it. It should be a, I think, for everyone, because they publish it, and they claim it to be of the first.\n",
            "A: I'm all i'm not sure.\n",
            "A: Do you install the application directly. Or do you go to Github and do when you pull the project?\n",
            "B: Okay, So I mean, then you would not know which program do you? Do you know his programming language, the software?\n",
            "Okay.\n",
            "B: And then\n",
            "B: you like what libraries these software might be using, and all that is, of course, that that most you don't need that information. Um, I never actually try to figure out what's inside.\n",
            "B: Yeah, but you know sort of how many people contribute uh. So So the only contributed on your project. Right? No.\n",
            "A: Oh, you mean I'm. I'm only responsible for my own on your for your research project is the only contributor. Right? Uh, technically yes. But I mean sometimes you cause other side of that. I require you to like, uh like to have a slight collaboration with other people\n",
            "A: for the life. I actually currently have a\n",
            "A: application that we can directly put to like isot home label into a middle of protein. First thing is like It's sort of like in, usually very difficult. But now in my product it should be\n",
            "A: easier, and we sort of uh from my P. I get a like a small collaboration with someone from the University of Toronto. They have a very good M. Our facility, for we we have to secure it, doing some like collaboration,\n",
            "A: the other than the and he's especially the I'm. I'm just i'm basically at work on my all.\n",
            "B: And do you know how many people contributed to the software that you are using? How many\n",
            "A: since I've been trained by the All the all the existing structural data. So I guess the country\n",
            "A: and the time, or I think that's also a call for our. So\n",
            "A: I don't know the exact information, but it should be just uh like, uh\n",
            "A: for that sort of group making all of the program.\n",
            "B: And do you know how old this this software is? When was it written?\n",
            "A: You know it. Just it. Basically doesn't. But it's a very new, very new view of how to use like Ai based program to credit\n",
            "A: is, I think I think they are pretty new.\n",
            "B: On On which machine do you typically run this on your local desktop or on a cluster\n",
            "I see.\n",
            "B: Uh. And so how heavy is this programs when you like running this program? Do you need to like only run this, or can you run other applications?\n",
            "A: Uh,\n",
            "A: I think they are pretty lightweight. I didn't. I didn't express. I didn't experience any problem with running, but I know some people was away all that I haven't used it, but I know I never\n",
            "A: does it require the gpu on your machine, or does it\n",
            "A: required gpu or something? But I I I don't I don't. I don't.\n",
            "A: Is, it seem to be actually consuming a lot of uh capacity as a computer. But I just don't know the exact number of it.\n",
            "B: Oh,\n",
            "A: uh,\n",
            "A: that's that sounds. I think that's pretty. It depends on the the the information I put in here like a small protein, will take much longer time. The B protein.\n",
            "A: Um, typically like, Are you stalled on your work because you're waiting for the software to finish the run?\n",
            "A: Uh? I don't really like a waste time or something. And you i'm not doing it. It's not like I I don't think it's affecting my computers as much. I can still do other job with it.\n",
            "B: Um! So you like fine with the speed of at which this program finishes because you always\n",
            "B: I see\n",
            "um.\n",
            "B: So you don't really need to optimize. And do you don't need this program to be optimized for you to be more efficient in your leisure? Yeah, I I think they're already pretty pretty well, and uh, you know, i'm using Kenya. I've thought that that's probably the reason why I don't have any issue with it.\n",
            "A: I I think I think that's probably like just coffee a lot with if I If you want to run something,\n",
            "B: I I I guess you Won't need any profile into your program, because you're fine with the speed of that. So that's\n",
            "B: um. So this is. Might be you Orthogonal, to your research. But are you a fair aware of the different types of parallelism\n",
            "your computer can offer you.\n",
            "A: So no, I\n",
            "A: never. It's all about that. Actually.\n",
            "Yeah, of course.\n",
            "B: Um, You, of course, are not using any parallelism in any, so that would make sense.\n",
            "B: Um! And do you ever use the cluster for anything? Do you already ever submit?\n",
            "A: I I know people from the like, the you know actual.\n",
            "A: I I don't know what these\n",
            "A: like uh\n",
            "B: you come across open Mp. Or P threads for multi-threading\n",
            "B: when you were learning\n",
            "B: has that. Have you come across?\n",
            "B: Yeah, um,\n",
            "uh,\n",
            "B: And you don't spend any time waiting on the processing of your data for projects, because that you have any number of tasks to do, anyway. So it does right. So like when you're running a program.\n",
            "Um,\n",
            "B: so you\n",
            "B: um\n",
            "A: all right. Do you use machine learning in your projects. Do I know?\n",
            "Ziyang Xu: Well, you use the indirectly because Alpha fold is like it is Ai based? Yeah, you are. You only like you only like, use it just like a another software. You're not like actively involving that.\n",
            "Ziyang Xu: But it's pretty cool that i'll actually get because this is really really new, right? Yeah, but it just It's just very effective. It's It's very good.\n",
            "A: So you like, I don't know why you guys can understand the like a lot of protein we use of have very difficulties like try to predict them, because previously the entire forty prediction is only based on, like the lowest energy\n",
            "A: which is not true in the nature of you, got Nature produce a amount of unstable protein for us, because I have a lot of functions, and also, for I could solve that problem because they just, they said it, learn from all the Us. And structural data, and they give out of a way to predict those structures. It's. It's very, very useful.\n",
            "Ziyang Xu: Actually, I have a a quick follow up on this is, so are there is offer of a fault to the only machine learning thing that you're using this research, Or are there like more machine learning based stuff that's getting\n",
            "A: into the field\n",
            "A: uh, like there's something called\n",
            "A: That's also like a structural predicting software and take a sort of make it, you know, like uh, make a structure for you. If you input on those sequels,\n",
            "A: and I think that's also involved on machine learning, because I need to know how to solve the protein.\n",
            "A: But uh, it's not really commonly used, because you don't really make something completely new. If that's that's for the If you make something that doesn't really make any sense in you. In the first place, that you will use it as a but if you are making something that is uh based on that natural stuff and just offer forward\n",
            "B: um. So would a research change. Let's say our current software that you use. They became like ten x faster, or that affect your\n",
            "A: I I don't think, because now it's already pretty efficient like I it doesn't really waste my time. So if they become much faster\n",
            "A: uh if I don't really gonna make an imperative,\n",
            "A: It's probably just because i'm not dealing with That's kind of super complicated, since now. So I don't have any like any like a requirement for a speed.\n",
            "B: Are there any language preferences that you have when it comes to like reading or writing code? If you need to do it?\n",
            "A: I don't know I I I never learn any longer. Also I will see. So I guess I don't have any preference. It just something new to learn right,\n",
            "A: and it's learning more. It's always better since you probably got to do something else outside of research with the coding. So I I don't have any problem with that.\n",
            "B: Um. So if if you're writing code, what features would you like your programming environment to have when you let's say using xcore I use using visual studio.\n",
            "A: Uh,\n",
            "A: I think it's probably gonna be very broad requirement like I want them to be. Have A very friend of the Ui\n",
            "A: is a You it?\n",
            "A: So you you you want. It looks like something from ninety. If you just make me feel very uncomfortable, it doesn't\n",
            "A: at least tell me like which function is like, Where is those function that I actually need is the instead of um that complaint nurses icon like this person that's lying around without any,\n",
            "A: I would say any indication that it's right there.\n",
            "A: I I think that's probably the the biggest thing that we want to have. I'll I Just\n",
            "A: I think that's what just make the entire learning process way much faster.\n",
            "A: I don't know if I have any common, not common, because since I don't really do coding, so\n",
            "A: it's it's it's more as a learning side.\n",
            "B: Um! Is there anything you would like from the Cs community? Let's say a better tools or better systems, easier programming languages.\n",
            "A: I don't know like,\n",
            "A: because that's that's also that you guys probably don't do a lot of stuff. So it hard for me to think about it.\n",
            "A: But still,\n",
            "Ziyang Xu: Yeah, for example, like\n",
            "Ziyang Xu: of of a fold. I feel like it changed the community a little bit. Uh, Is there anything else that you feel like? It's wasting a lot of your research time, even not not like computation related. But it could be automatic, automated by some tool\n",
            "Ziyang Xu: like. Is there anything that you feel like could be better if the Cs community get involved.\n",
            "A: Uh,\n",
            "A: maybe something.\n",
            "A: I think it's It's all about prediction stuff like uh\n",
            "A: crossing protection. One thing and right.\n",
            "A: I think all the i'm with molecular like I would say about the chemistry. Is it so? You always want to predict something?\n",
            "A: And definitely, if if we all this, like said people are in the service of that. Say radial chemistry stuff definitely. If they all develop like the same coding tool as It's the same tool like offer for it. It's way much better for a company community to work on, because,\n",
            "A: like crazy. So a lot of time you're just working in a black box. But now we have those prediction tools.\n",
            "A: We will have like a rough idea of what the exactly we we may go to get in this in, and we will have. We don't make for our job way much easier.\n",
            "A: It's nice thing is It's It's it's maybe\n",
            "A: I just don't use it to a lot of uh. It's a more or better, I guess it. We We always want to know what we make in the in the future like before we you started all the experiments, and we know which wise it's easier to get right, so they will save every save all the time and say over a month.\n",
            "Ziyang Xu: So uh, so pretty much for different, like kind of experiment. You do want uh some software to kind of predict that you will get like Alpha Fold, it can be for other other parts, not even protein\n",
            "A: uh folding. But for other like chemistry. Yeah, exactly like, for example, there's a lot of funny things. So I just kind of came out with the so in our extremely we love love on time. When we do pur vacation. We use uh several chromatography like Hpoc.\n",
            "A: If there's a software that I can. I've I've I've I've I've I've I've I've I can't recall anyone in the so when the protein is gonna come out under a certain condition in that column, it could be very helpful for me. I think that's something that I I don't. I don't. I can't recall anyone actually made that thing, but\n",
            "A: I I think it will be very helpful, especially. But you are dealing with something in some kind of very crazy protein That's very hard to get you from column. If there are software that can help me to predict that, then I will give it way. Much easier for me to develop a optimized master to actually purify such protein.\n",
            "A: It's all about predicting where it's going to resolve.\n",
            "I see\n",
            "A: that that's actually quite interesting. Yeah, that's you know. Actually, It's a you might undergrad in in email as a coding class. So there's all the profile. I to ask, uh, also to make like a a game,\n",
            "A: as I said, find it outside the final assignment. And what our group may be is the chromaton with simulator.\n",
            "A: It's it's It's actually coming to become a real thing. I I believe it will be like a very helpful for a certain group of people in the country chemistry community.\n",
            "No,\n",
            "that's\n",
            "A: that's cool. Yeah, um. So is there anything that you want us to ask? But we missed in all of these guys in the city as well. I've I've I think I've left to eager in the like in the computer science area. So it's very hard for me to actually think about it as\n",
            "B: no\n",
            "B: thanks so much. I didn't. That's all the questions that I had to ask.\n",
            "B: Really really helpful for us. Thank you for taking out all the time. And\n",
            "Ziyang Xu: Yup, This is really helpful, although, like the so it you might not have a lot of programming experience. But the fact that uh, you are actually dealing with, uh, like all the phone on the software, and you don't need to uh program that much to have a like a successful research. Actually, I feel\n",
            "A: i'm quite excited, because that's a goal for for us to to provide like. Maybe if you just tell back about ten years ago. Or maybe people just Don't trust computers. They think the computer computer cannot like deal with all all those complicated a single net. At that time people seeing nature. It's way too complicated for a human computer,\n",
            "A: but not just It's just say we have those like machine learning those The neural network is actually able to do That is, it is it's a very huge change in the mindset of a people, not people that's motivating to see more of those develop in the future because it does help a lot.\n",
            "A: Um, yeah, it's It's a very good,\n",
            "awesome, Thank you.\n",
            "Thank you so much. You\n",
            "Ziyang Xu: bye. Thank you.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(cleaned_transcripts[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eioO3cCOt1aT"
      },
      "outputs": [],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful research assistant. You will parse through an interview transcript and attempt to extract the respondent's answers to certain questions.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Here is a transcript of a research: {cleaned_transcripts[10]}. The interviewer, 'B' asks the interviewee 'A' the following question: '2.7 Do you usually write software to automate mundane tasks (like transferring data, making plots)?' Can you report the interviewee's response. Further, can you report the exact lines that the interviewee said to answer the question\" },\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqVYYJ9ht5oy",
        "outputId": "e8da0e23-7b61-47f5-ef62-fbb764e68875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The interviewee 'A' responded with: \n",
            "\n",
            "\"No, sure. I mean, I I have no mathematics of code it by using code very loosely here, so that I can basically just add the parameters and regenerate the same plot for presentations. But I wouldn't not in the manner you're describing.\"\n",
            "\n",
            "In the script above, these lines are:\n",
            "A: No, sure. I mean, I I have no mathematics of code it by using code very loosely here, so that I can basically just add the parameters and regenerate the same plot for presentations. But I wouldn't not in the manner you're describing.\n"
          ]
        }
      ],
      "source": [
        "print(response['choices'][0]['message']['content'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
